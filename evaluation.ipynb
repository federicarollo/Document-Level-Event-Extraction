{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db7c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, ast, copy, jsonschema\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from json import JSONDecodeError\n",
    "from fix_busted_json import repair_json\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad7cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from eval_script import multi_span_evaluate, count_overlap, compute_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949f96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import clean_italian_span, tokenize_italian_text, stops, italian_punctuation, check_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711609a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5fb3304-fa32-4512-8315-e955a219078a",
   "metadata": {},
   "source": [
    "## 1. Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f6011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nested_list(input_list):\n",
    "    return all(isinstance(el, list) for el in input_list)\n",
    "\n",
    "def is_str_list(input_list):\n",
    "    return all(isinstance(el, str) for el in input_list)\n",
    "\n",
    "def contains_no_list_elements(input_list):\n",
    "    return any(not isinstance(el, list) for el in input_list)\n",
    "\n",
    "def contains_list_elements(input_list):\n",
    "    return any(isinstance(el, list) for el in input_list)\n",
    "\n",
    "def contains_no_str_elements(input_list):\n",
    "    return any(not isinstance(el, str) for el in input_list)\n",
    "            \n",
    "def contains_str_elements(input_list):\n",
    "    return any(isinstance(el, str) for el in input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89057127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_nested_list(input_list):\n",
    "    \n",
    "    final_list = copy.deepcopy(input_list)\n",
    "    \n",
    "    while 1:\n",
    "        if not contains_list_elements(final_list):\n",
    "            break\n",
    "            \n",
    "        new_list = []\n",
    "        for i, el in enumerate(final_list):\n",
    "            if isinstance(el, list):\n",
    "                new_list += el\n",
    "            else:\n",
    "                new_list.append(el)\n",
    "        \n",
    "        final_list = new_list\n",
    "        \n",
    "    for i, span in enumerate(final_list):\n",
    "        final_list[i] = str(span)\n",
    "        \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8d7e20-356a-4bb4-8fff-d5928e48f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_nested_dictionary(input_dict):\n",
    "    \n",
    "    final_list = list(input_dict.values())\n",
    "    \n",
    "    while 1:\n",
    "        is_nested = False\n",
    "        for el in final_list:\n",
    "            if isinstance(el, dict):\n",
    "                is_nested = True\n",
    "\n",
    "        if not is_nested:\n",
    "            break\n",
    "\n",
    "        new_list = []\n",
    "        for i, el in enumerate(final_list):\n",
    "            if isinstance(el, dict):\n",
    "                new_list += list(el.values())\n",
    "            else:\n",
    "                new_list.append(el)\n",
    "                \n",
    "        final_list = new_list\n",
    "\n",
    "        \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f43a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dict_values(datum):\n",
    "    for tag in datum:\n",
    "        if isinstance(datum[tag], dict):\n",
    "            return False\n",
    "        for i, group in enumerate(datum[tag]):\n",
    "            if isinstance(group, list):\n",
    "                for j, span in enumerate(group):\n",
    "                    if isinstance(span, dict):\n",
    "                        return False\n",
    "            elif isinstance(group, dict):\n",
    "                return False\n",
    "                        \n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ef82c4-d866-4e25-8b4b-8ced04f6c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_none_values(datum):\n",
    "    for tag in datum:\n",
    "        if isinstance(datum[tag], list):\n",
    "            to_delete = []\n",
    "            for i, el in enumerate(datum[tag]):\n",
    "                \n",
    "                if isinstance(datum[tag][i], dict):\n",
    "                    for k, v in datum[tag][i].items():\n",
    "                        if v == None:\n",
    "                            return True\n",
    "                            \n",
    "                elif isinstance(datum[tag][i], list):\n",
    "                    to_delete2 = []\n",
    "                    \n",
    "                    for j, el2 in enumerate(datum[tag][i]):\n",
    "                        if isinstance(datum[tag][i][j], dict):\n",
    "                            for k, v in datum[tag][i][j].items():\n",
    "                                if v == None:\n",
    "                                    return True\n",
    "\n",
    "                        elif datum[tag][i][j] == None:\n",
    "                            return True\n",
    "                               \n",
    "                elif datum[tag][i] == None:\n",
    "                    return True\n",
    "                               \n",
    "        elif datum[tag] == None:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aeddd69-a0f4-4e96-b33e-0136e769f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_values(datum):\n",
    "    for tag in datum:\n",
    "        if isinstance(datum[tag], list):\n",
    "            to_delete = []\n",
    "            for i, el in enumerate(datum[tag]):\n",
    "                if isinstance(datum[tag][i], dict):\n",
    "                    \n",
    "                    to_delete2 = []\n",
    "                    for k, v in datum[tag][i].items():\n",
    "                        if v == None:\n",
    "                            to_delete2.append(k)\n",
    "\n",
    "                    for k in to_delete2:\n",
    "                        del datum[tag][i][k]\n",
    "                            \n",
    "                elif isinstance(datum[tag][i], list):\n",
    "                    \n",
    "                    to_delete2 = []\n",
    "                    for j, el2 in enumerate(datum[tag][i]):\n",
    "                        if isinstance(datum[tag][i][j], dict):\n",
    "                            \n",
    "                            to_delete3 = []\n",
    "                            for k, v in datum[tag][i][j].items():\n",
    "                                if v == None:\n",
    "                                    to_delete3.append(k)\n",
    "\n",
    "                            for k in to_delete3:\n",
    "                                del datum[tag][i][j][k]\n",
    "\n",
    "                        elif datum[tag][i][j] == None:\n",
    "                            to_delete2.append(j)\n",
    "\n",
    "                    to_delete2.reverse()\n",
    "                    for j in to_delete2:\n",
    "                        del datum[tag][i][j]\n",
    "                            \n",
    "                elif datum[tag][i] == None:\n",
    "                    to_delete.append(i)\n",
    "\n",
    "            to_delete.reverse()\n",
    "\n",
    "            for i in to_delete:\n",
    "                del datum[tag][i]\n",
    "                    \n",
    "                    \n",
    "        elif isinstance(datum[tag], dict):\n",
    "            to_delete = []\n",
    "            \n",
    "            for k, v in datum[tag].items():\n",
    "                if v == None:\n",
    "                    to_delete.append(k)\n",
    "\n",
    "            for k in to_delete:\n",
    "                del datum[tag][k]\n",
    "            \n",
    "        elif datum[tag] == None:\n",
    "            datum[tag] = []\n",
    "\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725e346-46ce-4ae7-aa3c-88607c2efbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd7eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dict_values(datum):\n",
    "    for tag in datum:\n",
    "        if isinstance(datum[tag], dict):\n",
    "            datum[tag] = flatten_nested_dictionary(datum[tag])\n",
    "            \n",
    "        for i, group in enumerate(datum[tag]):\n",
    "            if isinstance(group, list):\n",
    "                for j, span in enumerate(group):\n",
    "                    if isinstance(span, dict):\n",
    "                        datum[tag][i][j] = flatten_nested_dictionary(span)\n",
    "            else:\n",
    "                if isinstance(group, dict):\n",
    "                    datum[tag][i] = flatten_nested_dictionary(group)\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18620e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tag_values(datum):\n",
    "\n",
    "    for tag in available_tags:\n",
    "    \n",
    "        if tag in relation_tags:\n",
    "            if contains_no_list_elements(datum[tag]):\n",
    "                return False\n",
    "            else:\n",
    "                for group in datum[tag]:\n",
    "                    if contains_no_str_elements(group):\n",
    "                        return False\n",
    "        else:\n",
    "            if contains_no_str_elements(datum[tag]) or isinstance(datum[tag], str):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f0594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_tag_values(datum):\n",
    "    \n",
    "    for tag in available_tags:\n",
    "        \n",
    "        if tag in relation_tags:\n",
    "\n",
    "            if isinstance(datum[tag], str):\n",
    "                datum[tag] = [[datum[tag]]]\n",
    "            else:\n",
    "            \n",
    "                #relation tags\n",
    "                for i, group in enumerate(datum[tag]):\n",
    "                    if not isinstance(group, list):\n",
    "                        datum[tag][i] = [str(group)]\n",
    "                    else:\n",
    "                        datum[tag][i] = flatten_nested_list(datum[tag][i])\n",
    "        \n",
    "        else:\n",
    "            if isinstance(datum[tag], str):\n",
    "                datum[tag] = [datum[tag]]\n",
    "            else:\n",
    "                datum[tag] = flatten_nested_list(datum[tag])\n",
    "\n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3210a7-76f2-471a-bcc9-0c87b44e1293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce2d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema(datum):\n",
    "    try:\n",
    "        jsonschema.validate(datum, schema)\n",
    "            \n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        #print(\"Validation\")\n",
    "        return False\n",
    "    except jsonschema.exceptions.SchemaError as ex:\n",
    "        #print(\"Schema\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe390d2-48e6-4472-bf27-be6901e188e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e1f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_schema(datum):\n",
    "    \n",
    "    not_included_tags = [tag for tag in available_tags if tag not in datum]\n",
    "    \n",
    "    keys = list(datum.keys())\n",
    "    oos_tags = [key for key in keys if key not in available_tags]\n",
    "    \n",
    "    for tag in oos_tags:\n",
    "        del datum[tag]\n",
    "    \n",
    "    for tag in not_included_tags:\n",
    "        datum[tag] = []\n",
    "    \n",
    "    return datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aaaf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d743e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all_occurrences(text, to_replace, replace):\n",
    "    \n",
    "    occurrences = [m.start() for m in re.finditer(to_replace, text)]\n",
    "    occurrences.reverse()\n",
    "        \n",
    "    for i in occurrences:\n",
    "        text = text[:i] + replace + text[i+1:]\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff348e91-4b7b-4100-a961-7ba89bf38062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0cd3a7-7f13-4bf0-8035-c4fbaa667837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712dc3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:87: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:87: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7288\\3361395724.py:87: SyntaxWarning: invalid escape sequence '\\['\n",
      "  corrected_text = corrected_text.replace(\"\\[\", \"[\")\n"
     ]
    }
   ],
   "source": [
    "def get_json_from_string(text):\n",
    "    \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"″\", \"\\\"\")\n",
    "    \n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "\n",
    "    if start == -1 and end != -1:\n",
    "\n",
    "        start = 10000\n",
    "        for tag in available_tags:\n",
    "            index = text.find(f'\"{tag}')\n",
    "            if start > index and index != -1:\n",
    "                start = index\n",
    "                \n",
    "        corrected_text = \"{\" + text[start:end+1]\n",
    "        \n",
    "    elif start == -1 and end == -1:\n",
    "\n",
    "        if \"•\" in text:\n",
    "            result = {} \n",
    "            for tag in available_tags:\n",
    "                tag_index = text.find(tag)\n",
    "\n",
    "                if tag_index != -1:\n",
    "                    text_value = text[tag_index + len(tag) + 1:]\n",
    "\n",
    "                    endline_index = text_value.find(\"•\")\n",
    "                    if endline_index == -1:\n",
    "                        endline_index = text_value.find(\"\\n\")\n",
    "                    \n",
    "                    text_value = text_value[:endline_index]\n",
    "                    final_index = text_value.rfind(\"]\")\n",
    "\n",
    "                    if final_index != -1:\n",
    "                        text_value = text_value[:final_index+1]\n",
    "                        try:\n",
    "                            result[tag] = ast.literal_eval(text_value)\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "\n",
    "                    else:\n",
    "                        corrected_text = \"\"\n",
    "\n",
    "            if len(result) > 0:\n",
    "                return result, True\n",
    "            else:\n",
    "                corrected_text = \"\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            start = 10000\n",
    "            for tag in available_tags:\n",
    "                index = text.find(f'\"{tag}')\n",
    "                if start > index and index != -1:\n",
    "                    start = index\n",
    "            \n",
    "            #start = text.find('\"AUT')\n",
    "            end = text.rfind(']')\n",
    "    \n",
    "            if start < end:\n",
    "                corrected_text = \"{\" + text[start:end+1] + \"}\"\n",
    "\n",
    "            else:\n",
    "                corrected_text = \"\"\n",
    "            \n",
    "                \n",
    "    elif start != -1 and end == -1:\n",
    "        end = text.rfind(']')\n",
    "        corrected_text = text[start:end+1] + \"}\"\n",
    "\n",
    "    elif start != -1 and end != -1:\n",
    "        if start > end:\n",
    "            start = text.find('{')\n",
    "            end = text.rfind('}')\n",
    "            \n",
    "            if start > end:\n",
    "                return empty_value, False\n",
    "\n",
    "        \n",
    "        corrected_text = text[start:end+1]\n",
    "        #print(corrected_text)\n",
    "        corrected_text = replace_all_occurrences(corrected_text, \",]\", \"\")\n",
    "        corrected_text = replace_all_occurrences(corrected_text, \", ]\", \"\")\n",
    "        corrected_text = replace_all_occurrences(corrected_text, \"] \\\"\", \"],\")\n",
    "        corrected_text = corrected_text.replace(\"\\[\", \"[\")\n",
    "        corrected_text = corrected_text.replace(\"]\\\"]\", \"]]\")\n",
    "    \n",
    "    corrected_text = corrected_text.replace(\"\\\\_\", \"_\")\n",
    "    #print(corrected_text)\n",
    "\n",
    "    try:\n",
    "        result = json.loads(repair_json(corrected_text))\n",
    "        return result, True\n",
    "\n",
    "    except JSONDecodeError  as e:\n",
    "        #print(\"Error JSON\", e)\n",
    "        return empty_value, False\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(\"Exception\", e)\n",
    "        return empty_value, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eda0ad-fcd7-489c-aab0-0707f36b4f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51a6c6-5ba6-422a-b7b2-6ae0503d6c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f65382b1-1165-42e1-a96c-4e2c81895bc8",
   "metadata": {},
   "source": [
    "## 2.Schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f29c68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_tags = ('LOC', 'AUT', 'PAR', 'OBJ', 'VIC', 'AUTG', 'VICG') \n",
    "relation_tags = ('AUT', 'VIC', 'OBJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f41de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "                    tag: {\"type\": [\"array\", \"string\", \"object\"]} for tag in available_tags\n",
    "    },\n",
    "    \"required\": list(available_tags),\n",
    "    \"additionalProperties\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76048510",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_value = {\n",
    "                tag: [] for tag in available_tags\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73cb2e-8106-49ec-8303-bfc45d2123c0",
   "metadata": {},
   "source": [
    "## 3. Load data to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d2b88f3-768a-4188-a666-3bd01834db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mixtral-8x7B-Instruct-v0.1\"\n",
    "dataset = \"validation_set\"\n",
    "combination = (3, 0, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e18eaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(combination) == 0:\n",
    "    filename = f\"llms_predictions\\\\{dataset}\\\\{model}\\\\predicted_0.json\"\n",
    "else:\n",
    "    char_str = [str(index) for index in combination]\n",
    "    filename = f\"llms_predictions\\\\{dataset}\\\\{model}\\\\predicted_{len(combination)}_{''.join(char_str)}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cd5dbdb4-d81c-415d-ad95-f6ad28c080ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llms_predictions\\\\validation_set\\\\mixtral-8x7B-Instruct-v0.1\\\\predicted_4_3052.json'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c4d938ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as f:\n",
    "    predicted_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8af9683a-ef89-40b5-bdf1-c9f6ec814685",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data//{dataset}.json\", \"r\") as f:\n",
    "    annotated_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b64ded30-dc6a-4021-a9c1-b1a7cefd6acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8e44f-0964-499e-9ccb-df02c0a4354a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45d4a981-88a4-4803-90e5-1320c92f6460",
   "metadata": {},
   "source": [
    "## 4. Prediction Parse Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "64a39a7e-13e5-40f3-a800-910e5c130217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_predictions(predicted_data, annotated_data, start_index=0):\n",
    "\n",
    "    assert len(predicted_data) == len(annotated_data)\n",
    "    assert start_index < len(predicted_data)\n",
    "\n",
    "    not_flattened_predictions = {tag: [] for tag in available_tags}\n",
    "    not_flattened_groundtruth = {tag: [] for tag in available_tags}\n",
    "    \n",
    "    ### Mistral models\n",
    "    total_predictions = {}\n",
    "    total_groundtruth = {}\n",
    "    \n",
    "    groundtruth = {\n",
    "                        tag: {} for tag in available_tags\n",
    "    }\n",
    "    \n",
    "    predictions = {\n",
    "                        tag: {} for tag in available_tags\n",
    "    }\n",
    "    incorrect_outputs = []\n",
    "    not_formatable_jsons = []\n",
    "    invalid_jsons = []\n",
    "    not_correctables = []\n",
    "    \n",
    "    for i, el in enumerate(predicted_data[start_index:]):\n",
    "        index = start_index + i\n",
    "\n",
    "        last_occurrence = 0\n",
    "        first_index = 0\n",
    "        if model.startswith(\"mi\"):\n",
    "            last_occurrence = el['completion'].rfind('[/INST]')\n",
    "            first_index = last_occurrence + 7\n",
    "        \n",
    "        if last_occurrence == -1:\n",
    "            incorrect_outputs.append(index)\n",
    "            predicted_data[index]['predictions'] = empty_value\n",
    "            \n",
    "        else:\n",
    "            completion = el['completion'][first_index:]\n",
    "            value, is_json = get_json_from_string(completion)\n",
    "    \n",
    "            if value == empty_value:\n",
    "                if not is_json:\n",
    "                    not_formatable_jsons.append(index)\n",
    "                predicted_data[index]['predictions'] = empty_value\n",
    "            else:\n",
    "                predicted_data[index]['predictions'] = value\n",
    "    \n",
    "                try:\n",
    "    \n",
    "                    if check_none_values(predicted_data[index]['predictions']):\n",
    "                        predicted_data[index]['predictions'] = remove_none_values(predicted_data[index]['predictions'])\n",
    "    \n",
    "                        if check_none_values(predicted_data[index]['predictions']):\n",
    "                            predicted_data[index]['predictions'] = empty_value\n",
    "                    \n",
    "                    if not check_schema(predicted_data[index]['predictions']):\n",
    "                        invalid_jsons.append(index)\n",
    "                        predicted_data[index]['predictions'] = correct_schema(predicted_data[index]['predictions'])\n",
    "    \n",
    "                        if not check_schema(predicted_data[index]['predictions']):\n",
    "                            not_correctables.append(index) \n",
    "                            predicted_data[index]['predictions'] = empty_value\n",
    "    \n",
    "                    if not check_dict_values(predicted_data[index]['predictions']):\n",
    "                        corrected_predictions = correct_dict_values(predicted_data[index]['predictions'])\n",
    "                        predicted_data[index]['predictions'] = corrected_predictions\n",
    "    \n",
    "                        if not check_dict_values(predicted_data[index]['predictions']):\n",
    "                            not_correctables.append(index) \n",
    "                            predicted_data[index]['predictions'] = empty_value\n",
    "    \n",
    "                    if not check_tag_values(predicted_data[index]['predictions']):            \n",
    "                        corrected_predictions = correct_tag_values(predicted_data[index]['predictions'])\n",
    "                        predicted_data[index]['predictions'] = corrected_predictions\n",
    "    \n",
    "                        if not check_tag_values(corrected_predictions):\n",
    "                            not_correctables.append(index) \n",
    "                            predicted_data[index]['predictions'] = empty_value\n",
    "                            \n",
    "                \n",
    "                except jsonschema.exceptions.ValidationError as e:\n",
    "                    not_correctables.append(index)\n",
    "                    predicted_data[index]['predictions'] = empty_value\n",
    "    \n",
    "\n",
    "        for tag, elements in predicted_data[index]['predictions'].items():\n",
    "            if tag in available_tags:\n",
    "                not_flattened_predictions[tag].append(elements)\n",
    "                predictions[tag][index] = flatten_nested_list(elements)\n",
    "                total_predictions[f\"{tag}_{index}\"] = flatten_nested_list(elements)\n",
    "    \n",
    "        for tag, elements in annotated_data[index]['annotation'].items():\n",
    "            if tag in available_tags:\n",
    "                not_flattened_groundtruth[tag].append(elements)\n",
    "                groundtruth[tag][index] = flatten_nested_list(elements)\n",
    "                total_groundtruth[f\"{tag}_{index}\"] = flatten_nested_list(elements)\n",
    "\n",
    "\n",
    "    invalid_outputs = len(incorrect_outputs) + len(not_formatable_jsons) + len(not_correctables)\n",
    "\n",
    "    return total_predictions, total_groundtruth, not_flattened_predictions, not_flattened_groundtruth, invalid_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f0fdadc2-0e6b-46d4-ba82-b0af08e1df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predictions, total_groundtruth, not_flattened_predictions, not_flattened_groundtruth, invalid = parse_predictions(predicted_data, annotated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2118770-0003-45a7-b80c-589d3e8767b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb35abd5-7dcc-4ecd-a9ab-a67cd6d8b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation, _, _ = multi_span_evaluate(total_predictions, total_groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f6d8b6c-a518-4064-abbd-312dcc51274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'em_precision': 54.920212765957444,\n",
       " 'em_recall': 61.427863163113535,\n",
       " 'em_f1': 57.99204306108121,\n",
       " 'overlap_precision': 65.17546934869122,\n",
       " 'overlap_recall': 75.16472274003263,\n",
       " 'overlap_f1': 69.81458426319898}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdb5ed-1730-42ed-8145-5d1357377a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39878384-ff65-4b92-a894-2d8e00c923a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f51a0b8e-e1e1-4213-acf2-68e72c28860d",
   "metadata": {},
   "source": [
    "## 5. Linkage sets metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68b32fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_apostrophe(span):\n",
    "\n",
    "    tokenized = []\n",
    "    span_copy = copy.deepcopy(span)\n",
    "\n",
    "    if span == \"\" or span == \"'\":\n",
    "        return [\"\"]\n",
    "\n",
    "    if span_copy[0] == \"'\":\n",
    "        tokenized = [\"'\", span_copy[1:]]\n",
    "    else:\n",
    "        tokenized = [span_copy]\n",
    "\n",
    "    if tokenized[-1][-1] == \"'\":\n",
    "        tokenized[-1] = tokenized[-1][:-1]\n",
    "        tokenized.append(\"'\")\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9e24b4a-233a-4bb9-9b83-9fc119a48a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_annotation(to_clear):\n",
    "    to_return = []\n",
    "    \n",
    "    for to_tokenize in to_clear:\n",
    "        tokenized = [clean_italian_span(el).lower() for el in tokenize_italian_text(to_tokenize)]\n",
    "\n",
    "        not_apostrofe = []\n",
    "        for token in tokenized:\n",
    "            not_apostrofe += split_apostrophe(token)\n",
    "        to_return += [el for el in not_apostrofe if el != \"\" and el not in stops and el not in italian_punctuation]\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce73ec-8181-4cc6-98f6-987e03b4fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0d0016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8429aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def overall_sublist_distances(groundtruth, prediction, threshold=0):\n",
    "    total_similarity = 0.0\n",
    "\n",
    "    possible_pairs = []\n",
    "\n",
    "    groundtruth_copy = copy.deepcopy(groundtruth)\n",
    "    prediction_copy = copy.deepcopy(prediction)\n",
    "\n",
    "    for couple in product(groundtruth_copy, prediction_copy):\n",
    "        element1, element2 = couple\n",
    "        similarity = jaccard_similarity(set(clear_annotation(element1)), set(clear_annotation(element2)))\n",
    "        possible_pairs.append((element1, element2, similarity))\n",
    "    \n",
    "    possible_pairs.sort(key=lambda x: x[2])\n",
    "    possible_pairs.reverse()\n",
    "\n",
    "    final_pairs = []\n",
    "    for pair in possible_pairs:\n",
    "        if pair[0] in groundtruth_copy and pair[1] in prediction_copy:\n",
    "            if pair[2] > threshold:\n",
    "                groundtruth_copy.remove(pair[0])\n",
    "                prediction_copy.remove(pair[1])\n",
    "                final_pairs.append(pair)\n",
    "    \n",
    "    return final_pairs, groundtruth_copy, prediction_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5036a-f723-4d5a-b484-250546e0f2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a85161e5-0642-4705-abb6-e786d5d66abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "58aa5e81-276b-4400-8cb8-f2ef643efe34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_pairs = []\n",
    "total_false_negatives = []\n",
    "total_false_positives = []\n",
    "\n",
    "for tag in relation_tags:\n",
    "    for el1, el2 in zip(not_flattened_groundtruth[tag], not_flattened_predictions[tag]):\n",
    "        pairs, false_negatives, false_positives = overall_sublist_distances(el1, el2, threshold=threshold)\n",
    "        total_pairs += pairs\n",
    "        total_false_negatives += false_negatives\n",
    "        total_false_positives += false_positives\n",
    "\n",
    "values = [el[2] for el in total_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d017c8e6-edbe-411a-ba02-33ccea435593",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(total_pairs) / (len(total_pairs) + len(total_false_positives))\n",
    "recall = len(total_pairs) / (len(total_pairs) + len(total_false_negatives))\n",
    "\n",
    "f1_score = (2*precision*recall) / (precision + recall) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cf899866-bff2-49c1-922a-2a6aede1ccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.51\n",
      "Recall: 0.75\n",
      "F1-score: 0.60\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision:.2f}\\nRecall: {recall:.2f}\\nF1-score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29fac8e-db74-43d1-b514-d5426266bf5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48183c4f-cedb-4ca9-b27c-6ba1e634d31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b8591-96bb-4c58-9386-0b19a5047bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
